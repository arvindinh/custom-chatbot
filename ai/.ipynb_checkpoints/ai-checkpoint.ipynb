{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI package\n",
    "contains the implementation for the LLM models and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pipenv in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (2023.6.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from pipenv) (2023.5.7)\n",
      "Requirement already satisfied: setuptools>=67.0.0 in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from pipenv) (67.8.0)\n",
      "Requirement already satisfied: virtualenv-clone>=0.2.5 in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from pipenv) (0.5.7)\n",
      "Requirement already satisfied: virtualenv>=20.17.1 in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from pipenv) (20.23.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from virtualenv>=20.17.1->pipenv) (0.3.6)\n",
      "Requirement already satisfied: filelock<4,>=3.11 in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from virtualenv>=20.17.1->pipenv) (3.12.0)\n",
      "Requirement already satisfied: platformdirs<4,>=3.2 in c:\\users\\adinh\\appdata\\roaming\\python\\python311\\site-packages (from virtualenv>=20.17.1->pipenv) (3.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pipenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pytest...\n",
      "Resolving pytest...\n",
      "[    ] Installing...\n",
      "Adding pytest to Pipfile's [packages] ...\n",
      "[    ] Installing pytest...\n",
      "Installation Succeeded\n",
      "[    ] Installing pytest...\n",
      "[    ] Installing pytest...\n",
      "\n",
      "Building requirements...\n",
      "[    ] Locking...\n",
      "Resolving dependencies...\n",
      "[    ] Locking...\n",
      "[    ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[    ] Locking...\n",
      "[   =] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[=   ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "[ ===] Locking...\n",
      "[  ==] Locking...\n",
      "[   =] Locking...\n",
      "[   =] Locking...\n",
      "[  ==] Locking...\n",
      "[ ===] Locking...\n",
      "[====] Locking...\n",
      "[=== ] Locking...\n",
      "[==  ] Locking...\n",
      "[    ] Locking...\n",
      "[=   ] Locking...\n",
      "[==  ] Locking...\n",
      "[=== ] Locking...\n",
      "Success!\n",
      "[ ===] Locking...\n",
      "[ ===] Locking...\n",
      "\n",
      "Installing dependencies from Pipfile.lock (d6a67f)...\n",
      "To activate this project's virtualenv, run pipenv shell.\n",
      "Alternatively, run a command inside the virtualenv with pipenv run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating a Pipfile for this project...\n",
      "Pipfile.lock not found, creating...\n",
      "Locking [packages] dependencies...\n",
      "Locking [dev-packages] dependencies...\n",
      "Updated Pipfile.lock (922e82e69ac92d524e9aec65cbead9fdef4cdb3fcff8f459d8998bfd7bd6a67f)!\n"
     ]
    }
   ],
   "source": [
    "!pipenv install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this module will be exported as a package, \\_\\_init\\_\\_.py just needs to exist so the Python packaging mechanism knows where to look for functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing __init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a __init__.py\n",
    "print('Hello, world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is processing the training data(instruction manuals). I'll move over to the utils module to implement that method to make it accessible for other modules to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this step is done, I'll start implementing the embeddings/vector store steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll stay away from cloud based databases for now, such as Pinecone. To my knowledge, some in-memory databases are FAISS and Chroma. I'll do some more research to find some more. For now, I'll just use FAISS, which also offers efficient similarity search.\n",
    "\n",
    "\n",
    "Note: For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to use FAISS in conjunction with the util modules we created earlier, the pdf loaders and text splitters. We'll just just PyMuPDF and RecursiveCharacterTextSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env OPENAI_API_KEY=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDMA ePort G9 Quickstart Guide\n",
      "TOOLS REQUIRED FOR INSTALLATION\n",
      "11/32” Socket\n",
      "1/4” Socket\n",
      "12” Extension\n",
      "Ratchet\n",
      "Phillips Screwdriver\n",
      "Wire Cutters\n",
      "Power Drill*\n",
      "3/16” & 3/8” Drill Bit*\n",
      "Multi-diameter Step Drill Bit, \n",
      "1/4”-3/4”*\n",
      "#VVXUD0101910\n",
      "* Required for surface-mount installations\n",
      "Thank you for your purchase. Before you start, please read these instructions thoroughly, \n",
      "and then take a few moments to plan your installation. \n",
      "You must have a signed ePort Connect Services Contract and a bank account assigned to \n",
      "this device for it to be able to accept credit card transactions.  Please call USA Technologies \n",
      "Customer Care at 1.888.561.4748, if you need activation documents.\n",
      "G9 Telemeter\n",
      "Card Reader\n",
      "CANTALOUPE\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install pymupdf\n",
    "!pip install tiktoken\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../utils/loaders')\n",
    "sys.path.append('../utils/splitters')\n",
    "from pymupdf import PyMuPDF_Loader\n",
    "from recursive import RecursiveCharacter_TextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDF_Loader(pdf)\n",
    "data = loader.load_text()\n",
    "\n",
    "splitter = RecursiveCharacter_TextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")\n",
    "docs = splitter.split_data(data)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "query=\"What are the tools required to install the ePort G9\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that FAISS works in memory since we didn't use an external DB. We can also see that the modules I made for the pymupdf loader and recursive text splitters are importable and worked as intended. Now, I want to try using Deep Lake, which seems to be a local, persistent database option so I don't have to keep re-computing embeddings. I only need to run the training model once, so it stores it locally once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go back and test the different embeddings models after we get the OpenAI model working and organized in a module. So, my next steps are to create a new subpackage called \"embeddings\" and make a \"openai_embeddings\" module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#make the directory embeddings\n",
    "Path(\"embeddings\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing embeddings/openai_embeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a embeddings/openai_embeddings.py\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class OpenAI_Embeddings:\n",
    "    \"\"\"\n",
    "    A class to initialize a new embedder/vectorizer object based on the OpenAIEmbeddings wrapper from langchain.\n",
    "    It does have different methods to embed/vectorize the text, but we'll just use it as an object to pass in to the vector store class.\n",
    "    If interested, visit https://python.langchain.com/en/latest/modules/models/text_embedding.html or the associated github repository.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, api_key, model):\n",
    "        \"\"\"\n",
    "        Initializes a new instances of the embeddings object that can be passed in to a vector store.\n",
    "        The vector store will handle using the chosen embedder/vectorizer to convert and store it.\n",
    "        \n",
    "        :param api_key: valid openai ai key to call the api\n",
    "        :param model: default is \"text-embedding-ada-002\", visit https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "        \"\"\"\n",
    "        self.vectorizer = OpenAIEmbeddings(open_ai_key=api_key, model=model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'embeddings/__init__.py'\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a setup.py file in the root level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- --------\n",
      "aiohttp                 3.8.4\n",
      "aiosignal               1.3.1\n",
      "asttokens               2.2.1\n",
      "async-timeout           4.0.2\n",
      "attrs                   23.1.0\n",
      "backcall                0.2.0\n",
      "certifi                 2023.5.7\n",
      "charset-normalizer      3.1.0\n",
      "colorama                0.4.6\n",
      "comm                    0.1.3\n",
      "dataclasses-json        0.5.8\n",
      "debugpy                 1.6.7\n",
      "decorator               5.1.1\n",
      "exceptiongroup          1.1.1\n",
      "executing               1.2.0\n",
      "faiss-cpu               1.7.4\n",
      "frozenlist              1.3.3\n",
      "greenlet                2.0.2\n",
      "idna                    3.4\n",
      "iniconfig               2.0.0\n",
      "ipykernel               6.23.2\n",
      "ipython                 8.14.0\n",
      "jedi                    0.18.2\n",
      "jupyter_client          8.2.0\n",
      "jupyter_core            5.3.1\n",
      "langchain               0.0.200\n",
      "langchainplus-sdk       0.0.10\n",
      "marshmallow             3.19.0\n",
      "marshmallow-enum        1.5.1\n",
      "matplotlib-inline       0.1.6\n",
      "multidict               6.0.4\n",
      "mypy-extensions         1.0.0\n",
      "nest-asyncio            1.5.6\n",
      "numexpr                 2.8.4\n",
      "numpy                   1.24.3\n",
      "openai                  0.27.8\n",
      "openapi-schema-pydantic 1.2.4\n",
      "packaging               23.1\n",
      "parso                   0.8.3\n",
      "pickleshare             0.7.5\n",
      "pip                     23.1.2\n",
      "platformdirs            3.5.3\n",
      "pluggy                  1.0.0\n",
      "prompt-toolkit          3.0.38\n",
      "psutil                  5.9.5\n",
      "pure-eval               0.2.2\n",
      "pydantic                1.10.9\n",
      "Pygments                2.15.1\n",
      "PyMuPDF                 1.22.3\n",
      "pytest                  7.3.2\n",
      "python-dateutil         2.8.2\n",
      "pywin32                 306\n",
      "PyYAML                  6.0\n",
      "pyzmq                   25.1.0\n",
      "regex                   2023.6.3\n",
      "requests                2.31.0\n",
      "setuptools              67.7.2\n",
      "six                     1.16.0\n",
      "SQLAlchemy              2.0.16\n",
      "stack-data              0.6.2\n",
      "tenacity                8.2.2\n",
      "tiktoken                0.4.0\n",
      "tomli                   2.0.1\n",
      "tornado                 6.3.2\n",
      "tqdm                    4.65.0\n",
      "traitlets               5.9.0\n",
      "typing_extensions       4.6.3\n",
      "typing-inspect          0.9.0\n",
      "urllib3                 2.0.3\n",
      "wcwidth                 0.2.6\n",
      "wheel                   0.40.0\n",
      "yarl                    1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='ai',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing LLM model implementation for embedding models, chat models',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['langchain=0.0.200',\n",
    "                      'openai=0.27.8'\n",
    "                     ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll try out different embeddings models after I set up my vector store and prompting, since there's not really a gauge effectiveness of embedding models and I would also have to set up API keys for the other models as well. Now, I'll head back to the utils module and set up the local vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that it's set up, we have to try out and implement the different LLM models, as well as the Buffer Memory and Conversational Retrieval Chain. I'll do the latter first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll group the memory and convo retrieval chain into a subdirectory, \"chains\". More chains can be made based on use case, but I'll make a conversational.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"chains\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing chains/conversational.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a chains/conversational.py\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "class ConversationModel:\n",
    "    \"\"\"\n",
    "    ConversationModel is a simple wrapper for a conversational language model that uses chat history in addition to context from db\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, db):\n",
    "        \"\"\"\n",
    "        Initializes a conversational retrieval chain based on a given llm model, vector store.\n",
    "        \n",
    "        :param llm: langchain language model object\n",
    "        :param db: langchain vector store object\n",
    "        \"\"\"\n",
    "        memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "        self.chat = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), memory=memory)\n",
    "    \n",
    "    def get_response(self, query):\n",
    "        \"\"\"\n",
    "        returns the response given by the given language model based on a given query\n",
    "        \n",
    "        :param query: string, question to be passed in to the llm\n",
    "        \n",
    "        :return: string, response given by llm based on query and embedded documents in vector store\n",
    "        \"\"\"\n",
    "        response = self.chat({\"question\": query})\n",
    "        return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a subdirectory for LLM's. We'll start with a openai module in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"llms\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llms/openai.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a llms/openai.py\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "class Open_AI:\n",
    "    \"\"\"\n",
    "    Simple wrapper for regular OpenAI langchain class, can adjust temperature and pass in api key\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature = 0, api_key):\n",
    "        \"\"\"\n",
    "        Initializes an OpenAI object that can be passed in to a chain in a given chains module\n",
    "        \n",
    "        :param temperature:takes values 0-10, lower = more focused and deterministic, higher = random and diverse. \n",
    "        :param api_key: openai api key\n",
    "        \"\"\"\n",
    "        self.model = OpenAI(temperature = temperature, openai_api_key = api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick detour, I'll set up the training and prompting in their respective packages with the openai model. Then I'll start exploring other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll try AI21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a lot of API keys from here on, so I'll need to keep track of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%makefile` not found.\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a .env\n",
    "OPENAI_API_KEY='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'\n",
    "AI21_API_KEY='aSAEAEDJMK5gpPEgbwWbfwPrKzkpY9WX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping dotenv as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another detour! We need to refactor the code and our directorites, specifically the ai and embeddings subdirectories.I'm going to start with the embeddings. We can actually use a embeddings mapper similar to how we handled the text loaders. The model is going to be passed in by name and map to the corresponding embeddings model. For example, \"openai\" is going to map to langchain's OpenAIEmbeddings()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing embeddings/embeddings_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile embeddings/embeddings_mapper.py\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class Embeddings_Mapper:\n",
    "    \"\"\"\n",
    "    A class to initalize a new embedding model based on the wrappers from langchain.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "\n",
    "        self.model_map = {\n",
    "            \"openai\" : {OpenAIEmbeddings, {\"model\": \"text-embedding-ada-002\", \"openai_api_key\": self.api_key}}\n",
    "        }\n",
    "    \n",
    "    def find_embeddings_model(self, model):\n",
    "        if model in self.model_map:\n",
    "            model_class, model_args = self.model_map[model]\n",
    "            model = model_class(**model_args)\n",
    "            return model\n",
    "        \n",
    "        raise ValueError(f\"Model '{model}' not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"embeddings/openai_embeddings.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llms/llms_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile llms/llms_mapper.py\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "class LLMs_Mapper:\n",
    "    \"\"\"\n",
    "    A class to initalize a new language model based on the wrappers from langchain.\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "\n",
    "        self.model_map = {\n",
    "            \"openai\" : {OpenAI, {\"temperature\": 0, \"openai_api_key\": self.api_key}}\n",
    "        }\n",
    "    \n",
    "    def find__llm(self, model):\n",
    "        if model in self.model_map:\n",
    "            model_class, model_args = self.model_map[model]\n",
    "            model = model_class(**model_args)\n",
    "            return model\n",
    "        \n",
    "        raise ValueError(f\"LLM '{model}' not recognized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to make sure I can mix and match embeddings models, with llms, and etc. If not, I'll make specific combinations with the ones that work. I may put those in the chains dir afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import argparse\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from utils.loaders.loader_mapper import LoaderMapper\n",
    "from utils.splitters.recursive import RecursiveCharacter_TextSplitter\n",
    "from utils.vectorstores.deep_lake import DeeplakeDB\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "def load_and_split(pdf):\n",
    "    \"\"\"\n",
    "    This method takes an input pdf to be loaded and split into chunks\n",
    "    \n",
    "    :param pdf: path to training document\n",
    "    \n",
    "    :return: split langchain Document objects\n",
    "    \"\"\"\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(pdf)\n",
    "    data = loader.load()\n",
    "    # split extracted text(tokenize)\n",
    "    # split recursively by different characters - starting with \"\\n\\n\", then \"\\n\", then \" \"\n",
    "    splitter = RecursiveCharacter_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len\n",
    "    )\n",
    "    docs = splitter.split_data(data)\n",
    "    return docs\n",
    "\n",
    "def embed_and_store(docs):\n",
    "    \"\"\"\n",
    "    This method takes an input list of chunked documents to be embedded and stored\n",
    "    \n",
    "    :param docs: list of split langchain Document objects\n",
    "    \"\"\"\n",
    "    # initialize embeddings model to pass in to db\n",
    "    embeddings = HuggingFaceEmbeddings()\n",
    "    # initialize vector store, add split docs\n",
    "    # (db will compute embeddings using embedding model and store in specified path)\n",
    "    deeplake = DeeplakeDB(store_path='./embeddings_deeplake', embedding_model=embeddings)\n",
    "    deeplake.add_docs(docs)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    When file is run, command line takes input file paths separated by spaces. These will be loaded, split, and embedded, then stored.\n",
    "    \"\"\"\n",
    "    docs = ['../training/docs/ePortG11InstallGuide.pdf']\n",
    "    split_docs = []\n",
    "    for doc in docs:\n",
    "        chunks = load_and_split(doc)\n",
    "        split_docs.append(chunks)\n",
    "        \n",
    "    embed_and_store(split_docs)\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!del test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we know we can mix and match embeddings and llms, add the huggingface embeddings as an option to the mapper. Also, we can handle the api keys in this ai module since that's where it is actually used, so we don't have to worry about passing it in the prompting module or training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid switch - \"openai.py\".\n"
     ]
    }
   ],
   "source": [
    "%%writefile embeddings/embeddings_mapper\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "class Embeddings_Mapper:\n",
    "    \"\"\"\n",
    "    A class to initalize a new embedding model based on the wrappers from langchain.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.openai_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "        self.model_map = {\n",
    "            \"openai\" : {OpenAIEmbeddings, {\"model\": \"text-embedding-ada-002\", \"openai_api_key\": self.openai_key}},\n",
    "            \"huggingface\": {HuggingFaceEmbeddings, {}},\n",
    "        }\n",
    "    \n",
    "    def find_model(self, model):\n",
    "        if model in self.model_map:\n",
    "            model_class, model_args = self.model_map[model]\n",
    "            model = model_class(**model_args)\n",
    "            return model\n",
    "        \n",
    "        raise ValueError(f\"Model '{model}' not recognized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llms/llms_mapper.py\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "class LLMs_Mapper:\n",
    "    \"\"\"\n",
    "    A class to initalize a new language model based on the wrappers from langchain.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        initializes a new mapper to return a LLM object based on the langchain wrapper\n",
    "\n",
    "        \"\"\"\n",
    "        self.openai_key = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "        self.model_map = {\n",
    "            \"openai\" : {OpenAI, {\"temperature\": 0, \"openai_api_key\": self.openai_key}},\n",
    "            #temperature:takes values 0-10, lower = more focused and deterministic, higher = random and diverse. \n",
    "        }\n",
    "    \n",
    "    def find_model(self, model):\n",
    "        if model in self.model_map:\n",
    "            model_class, model_args = self.model_map[model]\n",
    "            model = model_class(**model_args)\n",
    "            return model\n",
    "        \n",
    "        raise ValueError(f\"LLM '{model}' not recognized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now head over to training and prompting modules to fix the imports and instantiations of the new classes we made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write up some tests for the embeddings, llms, and the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_embeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_embeddings.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pytest\n",
    "from ai.embeddings.embeddings_mapper import Embeddings_Mapper\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "@pytest.mark.parametrize(\"model, expected\", [\n",
    "    (\"openai\", OpenAIEmbeddings()),\n",
    "    (\"huggingface\", HuggingFaceEmbeddings()),\n",
    "])\n",
    "def test_mapper(model, expected):\n",
    "    mapper = Embeddings_Mapper()\n",
    "    embeddings = mapper.find_model(model)\n",
    "    assert type(embeddings) == type(expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.10.9, pytest-7.4.0, pluggy-1.2.0 -- C:\\Users\\adinh\\.virtualenvs\\ai-TGJVfiId\\Scripts\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\ai\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/test_embeddings.py::test_mapper[openai-expected0] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 50%]\u001b[0m\n",
      "tests/test_embeddings.py::test_mapper[huggingface-expected1] \u001b[32mPASSED\u001b[0m\u001b[32m      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 8.00s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v tests/test_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_llms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_llms.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pytest\n",
    "from ai.llms.llms_mapper import LLMs_Mapper\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"model, expected\", [\n",
    "    (\"openai\", OpenAI()),\n",
    "])\n",
    "def test_mapper(model, expected):\n",
    "    mapper = LLMs_Mapper()\n",
    "    llm = mapper.find_model(model)\n",
    "    assert type(llm) == type(expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.10.9, pytest-7.4.0, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\ai\n",
      "collected 1 item\n",
      "\n",
      "tests\\test_llms.py \u001b[32m.\u001b[0m\u001b[32m                                                     [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 2.17s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_llms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-TGJVfiId",
   "language": "python",
   "name": "ai-tgjvfiid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
