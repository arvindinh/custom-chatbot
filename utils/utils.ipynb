{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils package\n",
    "contains common reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a __init__.py\n",
    "print('Hello, world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this module will be exported as a package, \\_\\_init\\_\\_.py just needs to exist so the Python packaging mechanism knows where to look for functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with processing the training data(instruction manual PDFs). I want to try both the various langchain pdf readers(eg. PyPDF, MathPix, Unstructured, PyPDFium2, PDFMiner, PyMuPDF, pdfplumber) and the PyPDF2 library to see if there are any differences or advantages. I also want to try out the different text splitter parameters(eg. smaller chunks may sometimes be more likely to match a query, so try varying chunk sizes and overlap). There are also different options for text splitters(character, recursive character, NLTK, spaCy, Tiktoken, Hugging Face tokenizer, tiktoken(OpenAI) tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install pypdf2 langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, try processing using PyPDF2's PdfReader and langchain's CharacterTextSplitter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to use pipenv in conjunction with Jupyter, do the following steps:\n",
    "1. Navigate to project folder. In my case it was cd/Downloads/ctlpchatbot/utils.\n",
    "2. In your project folder, do pipenv install ipykernel\n",
    "                              pipenv shell\n",
    "  If the machine doesn't recognize pipenv, add python -m before running the commands.\n",
    "3. This will bring up a terminal in your virtualenv like this: (my-virtualenv-name) $\n",
    "4. In that shell do: python -m ipykernel install --user --name=my-virtualenv-name\n",
    "5. Launch jupyter notebook by running in the shell: jupyter notebook\n",
    "6. In your notebook, top bar, near top left, to the right of \"Cell\" and to the left of \"Widgets\"\n",
    "   Click Kernel -> Change Kernel. You should see the kernel my-vertualenv-name(whatever the name you set) as an option.\n",
    "   Select that as your kernel and the pipenv environment should work, allowing the imports to be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "\n",
    "#extract information from pages in pdf and combine into a singular raw text\n",
    "reader = PdfReader(pdf)\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results I got above are interesting. All I did was append the pages together, but it seems like some of the words are split for some reason. I'm not sure how much this will affect the results or querying but I also want to see what the text would look like with another pdf reader. After I try the text splitter, I'm going to try langchain's library for pdf readers to see if I get a similar result. It might just be the way the PDF is formatted rather than a fault of the pdf reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll try using solely langchain's libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MathPix requires an API key so I'll avoid that for now to avoid uncessesary charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Langchain's PyPDF wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyPDFLoader(pdf)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Unstructured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = UnstructuredPDFLoader(pdf)\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = UnstructuredPDFLoader(pdf)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just judging from looks, this text looks less fragmented than the one retrieved by PyPDF2. I don't see any words with unintended spaces in them. I also want to note here that you can also fetch remote PDFs using Unstructured, so if there are online pdf sites, the OnlinePDFLoader class can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyPDFium2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdfium2\n",
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyPDFium2Loader(pdf)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also looks slightly different from the last two. Some things seem to be in different order, for example the tootle required for installation came before \"Thank you for your purchase. Before you start, please read these instructions thoroughly\"..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PDFMiner:\n",
    "\n",
    "Note: This can be helpful for chunking texts semanticaly into sections as the output html content can be parsed via BeautifulSoup to get more structured and rich information about font size, page numbers, pdf headers/footers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PDFMinerPDFasHTMLLoader(pdf)\n",
    "data = loader.load()[0] #entire pdf is loaded as single doc\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs html code, so I don't think I'll be using it as a loader. But if you do want to parse the HTML using BeautifulSoup, it's at https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMuPDF:\n",
    "\n",
    "Note: According to langchain, this is the fastest of the PDF parsing options, contains detailed metadata about the PDF and its pages, as well as returns one document per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymupdf\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDFLoader(pdf)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more detailed than the prior options, as you can see the metadata tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pdfplumber:\n",
    "    \n",
    "Note: Similar to PyMuPDF, it also contains detailed metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PDFPlumberLoader(pdf)\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've finished exploring the pdf loaders offered by langchain, I don't see any inherently \"better\" options. But the ones offered by langchain do appear to be better than the PyPDF2 standalone library since there's no fragmented text. But I want to try them all anyways when it comes to chunking these documents, convert into embeddings, then to a vector store. We'll see the differences in quality of responses if there are any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'll probably be re-using these functions, I'll organize the different pdf loaders into classes, so they can be instantiated and used when necessary when this utils module is imported as a package. I'll have different functions, such as a load function and a separate one for the chunking stage.\n",
    "\n",
    "# First, I'll make the PyPDF2 class in the pypdf2.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a pypdf2.py\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "class PyPDF2_Reader:\n",
    "    \"\"\"\n",
    "    A class to read and extract text from PDF files using the PyPDF2 library\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of PyPDF2 PdfReader with the given filepath\n",
    "        \n",
    "        :param filepath: The path to the PDF file to be read\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.reader = PdfReader(filepath)\n",
    "    \n",
    "    def load_text(self):\n",
    "        \"\"\"\n",
    "        Loads and extracts the text content from the PDF file\n",
    "        \n",
    "        :return: The extracted text as a string\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        for page in self.reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We also have to make/update a setup.py file to keep track of the dependencies needed since this package will be exported and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipenv install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1', 'langchain=0.0.199', 'pytest=7.3.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we repeat these steps for the other pdf loaders in langchain's library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, before proceeding. I wanted to quickly organize the utils directory since it will become cluttered. I realized that there are going to be more modules than I thought, so I'll be organizing them into loaders and splitters. PDF Loader classes will be in loaders and text splitters will be in splitters. I can keep the setup.py file where it is since it's usually kept at the top-level, but I need to create a directory for loaders and move the pypdf2.py file there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#make the directories \"loaders\" and \"splitters\"\n",
    "Path(\"loaders\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"splitters\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.move('pypdf2.py', 'loaders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a \\_\\_init\\_\\_.py in the subpackages loaders and splitters for them to be recognized as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a loaders/__init__.py\n",
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a splitters/__init__.py\n",
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, I also want to take some time to test the PyPDF2 class I made.(I added a quick main method for testing but you can't see that here. I  just instantiated the class by doing reader = PyPDF2_Reader(\"example_path\") then used reader.load_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run loaders/pypdf2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll set up the module for the Unstructured loader in unstructured.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a loaders/pypdf.py\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "class PyPDF_Loader:\n",
    "    \"\"\"\n",
    "    A class to load a PDF file using the PyPDF PyPDFLoader wrapper from the langchain library.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of PyPDF PyPDFLoader with the given filepath. \n",
    "        \n",
    "        :param filepath: The path to the PDF file to be loaded\n",
    "        \n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.loader = PyPDFLoader(filepath)\n",
    "    \n",
    "    def load_text(self):\n",
    "        \"\"\"\n",
    "        Loads the text into the document format used downstream(by text splitters, etc)\n",
    "        \n",
    "        :return: The pdf as a a Document(contains page_content and metadata)\n",
    "        \"\"\"\n",
    "        data = self.loader.load()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1', 'langchain=0.0.199', 'pytest=7.3.2', 'pypdf=3.9.1']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll set up the module for the Unstructured loader in unstructured.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a loaders/unstructured.py\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "class UnstructuredPDF_Loader:\n",
    "    \"\"\"\n",
    "    A class to load a PDF file using the UnstructuredPDFLoader wrapper from the langchain library.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of Unstructured UnstructuredPDFLoader with the given filepath. \n",
    "        \n",
    "        :param filepath: The path to the PDF file to be loaded\n",
    "        \n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.loader = UnstructuredPDFLoader(filepath)\n",
    "    \n",
    "    def load_text(self):\n",
    "        \"\"\"\n",
    "        Loads the text into the document format used downstream(by text splitters, etc)\n",
    "        \n",
    "        :return: The pdf as a a Document(contains page_content and metadata)\n",
    "        \"\"\"\n",
    "        data = self.loader.load()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the setup.py file to include the unstructured dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1', 'langchain=0.0.199', 'pytest=7.3.2', 'pypdf=3.9.1', 'unstructured=0.7.4']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll set up the module for the PyPDFium2 loader in pypdfium2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a loaders/pypdfium2.py\n",
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "\n",
    "class PyPDFium2PDF_Loader:\n",
    "    \"\"\"\n",
    "    A class to load a PDF file using the PyPDFium2 wrapper from the langchain library.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of PyPDFium2 PyPDFium2Loader with the given filepath. \n",
    "        \n",
    "        :param filepath: The path to the PDF file to be loaded\n",
    "        \n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.loader = PyPDFium2Loader(filepath)\n",
    "    \n",
    "    def load_text(self):\n",
    "        \"\"\"\n",
    "        Loads the text into the document format used downstream(by text splitters, etc)\n",
    "        \n",
    "        :return: The pdf as a a Document(contains page_content and metadata)\n",
    "        \"\"\"\n",
    "        data = self.loader.load()\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1', 'langchain=0.0.199', 'pytest=7.3.2', 'pypdf=3.9.1', 'unstructured=0.7.4', 'pypdfium2=4.15.0', 'pdf2image=1.16.3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll set up the module for the PyMuPDF loader in pymupdf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a loaders/pymupdf.py\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "class PyMuPDF_Loader:\n",
    "    \"\"\"\n",
    "    A class to load a PDF file using the PyMuPDF wrapper from the langchain library.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of PyMuPDF PyMuPDFLoader with the given filepath. \n",
    "        \n",
    "        :param filepath: The path to the PDF file to be loaded\n",
    "        \n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.loader = PyMuPDFLoader(filepath)\n",
    "    \n",
    "    def load_text(self):\n",
    "        \"\"\"\n",
    "        Loads the text into the document format used downstream(by text splitters, etc)\n",
    "        \n",
    "        :return: The pdf as a a Document(contains page_content and metadata)\n",
    "        \"\"\"\n",
    "        data = self.loader.load()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1',\n",
    "                      'langchain=0.0.199',\n",
    "                      'pytest=7.3.2',\n",
    "                      'pypdf=3.9.1',\n",
    "                      'unstructured=0.7.4',\n",
    "                      'pypdfium2=4.15.0',\n",
    "                      'pdf2image=1.16.3',\n",
    "                      'PyMuPDF=1.22.3'\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'll set up the module for the pdfplumber loader in pdfplumber.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a loaders/pdfplumber.py\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "\n",
    "class pdfplumber_loader:\n",
    "    \"\"\"\n",
    "    A class to load a PDF file using the pdfplumber wrapper from the langchain library.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of pdfplumber PDFPlumberLoader with the given filepath. \n",
    "        \n",
    "        :param filepath: The path to the PDF file to be loaded\n",
    "        \n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.loader = PDFPlumberLoader(filepath)\n",
    "    \n",
    "    def load_text(self):\n",
    "        \"\"\"\n",
    "        Loads the text into the document format used downstream(by text splitters, etc)\n",
    "        \n",
    "        :return: The pdf as a a Document(contains page_content and metadata)\n",
    "        \"\"\"\n",
    "        data = self.loader.load()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1',\n",
    "                      'langchain=0.0.199',\n",
    "                      'pytest=7.3.2',\n",
    "                      'pypdf=3.9.1',\n",
    "                      'unstructured=0.7.4',\n",
    "                      'pypdfium2=4.15.0',\n",
    "                      'pdf2image=1.16.3',\n",
    "                      'PyMuPDF=1.22.3',\n",
    "                      'pdfplumber=0.9.0'\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we'll start working with text splitters, which handle tokenization\n",
    "\n",
    "Query + Context -> LLM\n",
    "\n",
    "But need to think about max token limit of the LLM and how much space can be reserved for contexts?\n",
    "\n",
    "Example, if the model has 4096 available space for tokens, you need to think about both the input(query, context, instructions, history) and the output tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Text Splitter:\n",
    "\n",
    "Note: Simplest method, splits based on characters (by default \"\\n\\n\") and measure chunk length by number of characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.pymupdf import PyMuPDF_Loader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDF_Loader(pdf)\n",
    "\n",
    "data = loader.load_text()\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator= \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK:\n",
    "\n",
    "Note: text is split by NLTK tokenizer, chunk size measured by number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "from loaders.pypdf import PyPDF_Loader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyPDF_Loader(pdf)\n",
    "data = loader.load_text()\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Character:\n",
    "\n",
    "Note: recommeneded one for generic text. Parameterized by a list of characters[\"\\n\\n\", \"\\n\", \" \", \"\"]. This tries to keep the paragraphs, sentences, words together. Chunk size is measured by number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.pymupdf import PyMuPDF_Loader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDF_Loader(pdf)\n",
    "\n",
    "data = loader.load_text()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken:\n",
    "\n",
    "Note: fast BPE tokenizer created by OpenAI, text is split and chunk size is measured by tiktoken tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "from loaders.pymupdf import PyMuPDF_Loader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDF_Loader(pdf)\n",
    "\n",
    "data = loader.load_text()\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, I'll create the modules for each text splitter class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a splitters/character.py\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "class Character_TextSplitter:\n",
    "    \"\"\"\n",
    "    A class to split a Document using the CharacterTextSplitter wrapper from the langchain library\n",
    "    \"\"\"\n",
    "    def __init__(self, separator, chunk_size, chunk_overlap, length_function):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of CharacterTextSplitter\n",
    "        \n",
    "        :param separator: list of separator characters for the text splitter\n",
    "        :param chunk_size: Maximum size of chunks to return\n",
    "        :param chunk_overlap: Overlap in characters between chunks\n",
    "        :param length_function: Function that measures the length of given chunks\n",
    "        \n",
    "        \"\"\"\n",
    "        self.splitter = CharacterTextSplitter(\n",
    "            separator = separator,\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "            length_function = length_function\n",
    "        )\n",
    "    \n",
    "    def split_data(self, data):\n",
    "        \"\"\"\n",
    "        Splits the given Document based on single characters, default \"\\n\\n\", and measures chunk length by number of characters\n",
    "        \n",
    "        :param data: The Document to be split, in the Document format returned by the langchain pdf loaders\n",
    "\n",
    "        :return: Split Documents\n",
    "        \"\"\"\n",
    "        docs = self.splitter.split_documents(data)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a splitters/nltk.py\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "class NLTK_TextSplitter:\n",
    "    \"\"\"\n",
    "    A class to split a Document using the NLTKTextSplitter wrapper from the langchain library\n",
    "    \"\"\"\n",
    "    def __init__(self, chunk_size):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of NLTKTextSplitter\n",
    "        \n",
    "        :param chunk_size: Maximum size of chunks to return\n",
    "        \n",
    "        \"\"\"\n",
    "        self.splitter = NLTKTextSplitter(chunk_size = chunk_size)\n",
    "    \n",
    "    def split_data(self, data):\n",
    "        \"\"\"\n",
    "        Splits the given Document based on NLTK tokenzer, chunk size is measured by number of characters\n",
    "        \n",
    "        :param data: The Document to be split, in the Document format returned by the langchain pdf loaders\n",
    "\n",
    "        :return: Split Documents\n",
    "        \"\"\"\n",
    "        docs = self.splitter.split_documents(data)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the setup.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1',\n",
    "                      'langchain=0.0.199',\n",
    "                      'pytest=7.3.2',\n",
    "                      'pypdf=3.9.1',\n",
    "                      'unstructured=0.7.4',\n",
    "                      'pypdfium2=4.15.0',\n",
    "                      'pdf2image=1.16.3',\n",
    "                      'PyMuPDF=1.22.3',\n",
    "                      'pdfplumber=0.9.0',\n",
    "                      'nltk=3.8.1'\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a splitters/recursive.py\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class RecursiveCharacter_TextSplitter:\n",
    "    \"\"\"\n",
    "    A class to split a Document using the RecursiveCharacterTextSplitter wrapper from the langchain library.\n",
    "    Recommended text splitter for generic text.\n",
    "    \"\"\"\n",
    "    def __init__(self,, chunk_size, chunk_overlap, length_function):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of RecursiveCharacterTextSplitter\n",
    "        \n",
    "        :param chunk_size: Maximum size of chunks to return\n",
    "        :param chunk_overlap: Overlap in characters between chunks\n",
    "        :param length_function: Function that measures the length of given chunks\n",
    "        \n",
    "        \"\"\"\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "            length_function = length_function\n",
    "        )\n",
    "    \n",
    "    def split_data(self, data):\n",
    "        \"\"\"\n",
    "        Splits the given Document based on list of characters, [\"\\n\\n\", \"\\n\", \" \", \"\"]. Chunk size is measured of characters.\n",
    "        \n",
    "        :param data: The Document to be split, in the Document format returned by the langchain pdf loaders\n",
    "\n",
    "        :return: Split Documents\n",
    "        \"\"\"\n",
    "        docs = self.splitter.split_documents(data)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a splitters/tiktoken.py\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "class RecursiveCharacter_TextSplitter:\n",
    "    \"\"\"\n",
    "    A class to split a Document using the TokenTextSplitter wrapper from the langchain library.\n",
    "    \"\"\"\n",
    "    def __init__(self,, chunk_size, chunk_overlap):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of TokenTextSplitter\n",
    "        \n",
    "        :param chunk_size: Maximum size of chunks to return\n",
    "        :param chunk_overlap: Overlap in characters between chunks\n",
    "        \n",
    "        \"\"\"\n",
    "        self.splitter = TokenTextSplitter(\n",
    "            chunk_size = chunk_size,\n",
    "            chunk_overlap = chunk_overlap,\n",
    "        )\n",
    "    \n",
    "    def split_data(self, data):\n",
    "        \"\"\"\n",
    "        Splits the given Document based on tiktoken tokens. The text is split and chunk size is measured by tiktoken tokens.\n",
    "        \n",
    "        :param data: The Document to be split, in the Document format returned by the langchain pdf loaders\n",
    "\n",
    "        :return: Split Documents\n",
    "        \"\"\"\n",
    "        docs = self.splitter.split_documents(data)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update setup.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1',\n",
    "                      'langchain=0.0.199',\n",
    "                      'pytest=7.3.2',\n",
    "                      'pypdf=3.9.1',\n",
    "                      'unstructured=0.7.4',\n",
    "                      'pypdfium2=4.15.0',\n",
    "                      'pdf2image=1.16.3',\n",
    "                      'PyMuPDF=1.22.3',\n",
    "                      'pdfplumber=0.9.0',\n",
    "                      'nltk=3.8.1',\n",
    "                      'tiktoken=0.4.0'\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have our util functions for processing/loading the PDFs to the langchain Document format and splitting/tokenizing text, we can move on embeddings/vector stores phase. Since I will be using the OpenAI Embeddings, I'll move to the ai module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coming back from the ai module after implementing the openai_embeddings module, I'll now be implementing Deep Lake as my local vector store. It has the capacity to take in an Embeddings model and a Document as parameters, and both compute and store the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeplake\n",
    "!pip install openai\n",
    "import sys\n",
    "sys.path.append('../ai/embeddings')\n",
    "from openai_embeddings import OpenAI_Embeddings\n",
    "from loaders.pymupdf import PyMuPDF_Loader\n",
    "from splitters.recursive import RecursiveCharacter_TextSplitter\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDF_Loader(pdf)\n",
    "data = loader.load_text()\n",
    "splitter = RecursiveCharacter_TextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")\n",
    "docs = splitter.split_data(data)\n",
    "\n",
    "embeddings = OpenAI_Embeddings(api_key='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').vectorizer\n",
    "\n",
    "db = DeepLake(dataset_path=\"./documents_deeplake\", embedding_function=embeddings)\n",
    "db.add_documents(docs)\n",
    "query=\"What are the tools required to install the ePort G9\"\n",
    "response=db.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "db.delete_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that I know it works, I'll create a module for Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#make the directories \"loaders\" and \"splitters\"\n",
    "Path(\"vectorstores\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename= 'vectorstores/__init__.py'\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a vectorstores/deep_lake.py\n",
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "class DeeplakeDB:\n",
    "    \"\"\"\n",
    "    A class to initialize the Deep Lake vector store and perform various operations based on the DeepLake wrapper from langchain\n",
    "    \"\"\"\n",
    "    def __init__(self, store_path, embedding_model):\n",
    "        \"\"\"\n",
    "        Initializes the DeepLake object based on a given dataset path and embedding function/model.\n",
    "        DeepLake wrapper is capable of internally computing the embedding using the given model and storing it in the path.\n",
    "        \n",
    "        :param store_path: path that contains vector store. will create at that path if doesn't already exist \n",
    "        :param embedding_model: langchain embedding model\n",
    "        \"\"\"\n",
    "        self.db = DeepLake(dataset_path = store_path, embedding_function = embedding_model)\n",
    "\n",
    "    def add_docs(self, documents):\n",
    "        \"\"\"\n",
    "        Adds the embedded documents to the path given on initialization.\n",
    "        \n",
    "        :param document: langchain Document object used for computing embedding, then to be stored\n",
    "        \"\"\"\n",
    "        for document in documents:\n",
    "            self.db.add_documents(document)\n",
    "    \n",
    "    def find_similar(self, query):\n",
    "        \"\"\"\n",
    "        Returns the document that best matches the query\n",
    "        \n",
    "        :param query: String that is tested for similarity search\n",
    "        \n",
    "        :return: most similar Document object\n",
    "        \"\"\"\n",
    "        return self.db.similarity_search(query)\n",
    "\n",
    "    def delete_all(self):\n",
    "        \"\"\"\n",
    "        Deletes the vector store in the given path.\n",
    "        \"\"\"\n",
    "        self.db.delete_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update setup.py accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name='utils',\n",
    "    version='1.0',\n",
    "    author='Arvin Dinh',\n",
    "    description='A Python package containing common utility functions such as pdf reading, loading, and splitting into chunks',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['PyPDF2=3.0.1',\n",
    "                      'langchain=0.0.199',\n",
    "                      'pytest=7.3.2',\n",
    "                      'pypdf=3.9.1',\n",
    "                      'unstructured=0.7.4',\n",
    "                      'pypdfium2=4.15.0',\n",
    "                      'pdf2image=1.16.3',\n",
    "                      'PyMuPDF=1.22.3',\n",
    "                      'pdfplumber=0.9.0',\n",
    "                      'nltk=3.8.1',\n",
    "                      'tiktoken=0.4.0',\n",
    "                      'deeplake=3.6.3'\n",
    "                     ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just want to run a quick test of all the modules I created, so I'll import those and use them to store the computed embeddings in the vector store. Then I'll use that vector store as my retriever in the conversation chain to test if the LLM can produce a suitable reponse based on that stored embedding as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../ai/embeddings')\n",
    "from openai_embeddings import OpenAI_Embeddings\n",
    "from loaders.pymupdf import PyMuPDF_Loader\n",
    "from splitters.recursive import RecursiveCharacter_TextSplitter\n",
    "from vectorstores.deep_lake import DeeplakeDB\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "pdf = '../../ePortG9QuickStartGuide.pdf'\n",
    "loader = PyMuPDF_Loader(pdf)\n",
    "data = loader.load_text()\n",
    "splitter = RecursiveCharacter_TextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")\n",
    "docs = splitter.split_data(data)\n",
    "\n",
    "embeddings = OpenAI_Embeddings(api_key='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').vectorizer\n",
    "\n",
    "db = DeeplakeDB(store_path='./embeddings_deeplake', embedding_model=embeddings)\n",
    "db.add_docs(docs)\n",
    "llm = ChatOpenAI(openai_api_key='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')\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.db.as_retriever(), memory=memory)\n",
    "\n",
    "query = \"What tools do I need to install the ePort G9\"\n",
    "result = qa({\"question\": query})\n",
    "print(result[\"answer\"])\n",
    "\n",
    "db.delete_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It recognized the asterisk next to the words as corresponding labels for the \"Required for surface-mount installations\" tag. For example, it said Power Drill* and Multi-diameter Step Drill Bit* and at the bottom has *Required for surface-mount installations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's really intelligent and good at understanding text. Next, I'll be implementing the ChatOpenAI model and the conversation wrappers in the ai module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, I'll set up and organize tests with pytest. First, create a tests directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#make the directory \"pdf\"\n",
    "Path(\"loaders/pdf\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loaders/pdf\\\\unstructured.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.move('loaders/pdfplumber.py', 'loaders/pdf')\n",
    "shutil.move('loaders/pymupdf.py', 'loaders/pdf')\n",
    "shutil.move('loaders/pypdf.py', 'loaders/pdf')\n",
    "shutil.move('loaders/pypdfium2.py', 'loaders/pdf')\n",
    "shutil.move('loaders/unstructured.py', 'loaders/pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's make a main pdf loader module to handle the different file types and load accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing loaders/multi_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loaders/loader_mapper.py\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to loaders/multi_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a loaders/loader_mapper.py\n",
    "\n",
    "class LoaderMapper:\n",
    "    \"\"\"\n",
    "    LoaderMapper can accept multiple file types and return a langchain loader wrapper that corresponds to the associated loader.\n",
    "    Currently supports csv, pdf, txt, html, md, doc, docx, ppt, pptx, xls, xlsx, json\n",
    "    \"\"\"\n",
    "    #keep dict of file extensions and their relevant loaders with their arguments\n",
    "    loader_map = {\n",
    "            \".csv\": (CSVLoader, {}),\n",
    "            \".pdf\": (PyMuPDFLoader, {}),\n",
    "            \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "            \".html\": (UnstructuredHTMLLoader, {}),\n",
    "            \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "            \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "            \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "            \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "            \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "            \".xls\": (UnstructuredExcelLoader, {}),\n",
    "            \".xlsx\": (UnstructuredExcelLoader, {}),\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def find_loader(self, filepath):\n",
    "        \"\"\"\n",
    "        Finds the associated loader based on filepath extension\n",
    "        \n",
    "        :param filepath: path of the file to be loaded\n",
    "        \n",
    "        :return: langchain loader wrapper object\n",
    "        \"\"\"\n",
    "        ext = \".\" + filepath.rsplit(\".\", 1)[-1]\n",
    "        if ext in LoaderMapper.loader_map:\n",
    "            loader_class, loader_args = LoaderMapper.loader_map[ext]\n",
    "            loader = loader_class(filepath, **loader_args)\n",
    "            return loader\n",
    "        \n",
    "        raise ValueError(f\"Unsupported file extension '{ext}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test all the different file types to see if the class works as intended. We'll use the test_loaders.py module and pytest again. We also need to update the import line since we moved the pdf loaders under loaders/pdf. UPDATE: I just made a change to the multi_loader class and changed it to LoaderMapper instead. I want to return the relevant loader instead of doing all the loading work in that class. So I think I'll be removing the pdf directory altogether since we only need one pdf loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "directory = 'loaders/pdf'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory):\n",
    "    # Remove the directory and its contents\n",
    "    shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the old tests with the pdf loaders and write new ones. One for returning the correct loader and another for actually returning the loaded text. Parametrize for each file type: pdf, json, docx etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_loaders.py\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")\n",
    "from loaders.loader_mapper import LoaderMapper\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"doc, expected\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf', PyMuPDFLoader('tests/docs/dummy_doc_twinkle.pdf'))\n",
    "])\n",
    "def test_return_loader(doc, expected):\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(doc)\n",
    "    assert type(loader) == type(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 1 item\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 2.00s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/test_loaders.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more tests for each file type. I'm going to need to make dummy files for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tests/docs\\\\dummy_doc_twinkle.pdf'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(\"tests/docs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import csv\n",
    "\n",
    "data = [\n",
    "    ['Name', 'Age', 'Country'],\n",
    "    ['John', 25, 'USA'],\n",
    "    ['Alice', 30, 'Canada'],\n",
    "    ['Bob', 35, 'Australia']\n",
    "]\n",
    "\n",
    "filename = 'tests/docs/example.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n",
    "import shutil\n",
    "shutil.move('tests/dummy_doc_twinkle.pdf', 'tests/docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"tests/docs/dummy.txt\"\n",
    "\n",
    "content=\"\"\"Blah blah blah. Sample text. Blah Blah\n",
    "Blah Blah Blah. This is so fun. Blah Blah.\n",
    "Abcdefghijklmnopqrstuvwxyz.\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as file: \n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/docs/dummy.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/docs/dummy.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Dummy HTML File</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>This is a dummy HTML file.</h1>\n",
    "    <p>It serves as an example.</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/docs/dummy.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/docs/dummy.md\n",
    "\n",
    "# Dummy Markdown File\n",
    "\n",
    "This is a dummy Markdown file.\n",
    "It serves as an example.\n",
    "\n",
    "- Item 1\n",
    "- Item 2\n",
    "- Item 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Dummy Document', 0)\n",
    "document.add_paragraph('This is a dummy Word document.')\n",
    "\n",
    "document.save('tests/docs/dummy.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Dummy Document', 0)\n",
    "paragraph = document.add_paragraph('This is a dummy Word document.')\n",
    "paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "\n",
    "document.save('tests/docs/dummy.doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#powerpoint\n",
    "from pptx import Presentation\n",
    "\n",
    "presentation = Presentation()\n",
    "\n",
    "slide_1 = presentation.slides.add_slide(presentation.slide_layouts[0])\n",
    "title_1 = slide_1.shapes.title\n",
    "subtitle_1 = slide_1.placeholders[1]\n",
    "\n",
    "title_1.text = \"Dummy Presentation\"\n",
    "subtitle_1.text = \"This is a dummy PowerPoint presentation.\"\n",
    "\n",
    "presentation.save(\"tests/docs/dummy.pptx\")\n",
    "\n",
    "#excel\n",
    "import openpyxl\n",
    "\n",
    "workbook = openpyxl.Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "worksheet.title = \"Dummy Sheet\"\n",
    "worksheet[\"A1\"] = \"Dummy Excel Spreadsheet\"\n",
    "worksheet[\"A2\"] = \"This is a dummy Excel spreadsheet.\"\n",
    "\n",
    "workbook.save(\"tests/docs/dummy.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "# Create a presentation object\n",
    "presentation = Presentation()\n",
    "\n",
    "# Add a slide with a title and content\n",
    "slide_layout = presentation.slide_layouts[1]  # Slide layout with title and content\n",
    "slide = presentation.slides.add_slide(slide_layout)\n",
    "title = slide.shapes.title\n",
    "content = slide.placeholders[1]\n",
    "\n",
    "title.text = \"Dummy Slide\"\n",
    "content.text = \"This is a dummy PowerPoint slide.\"\n",
    "\n",
    "# Save the presentation to a PPT file\n",
    "presentation.save(\"tests/docs/dummy.ppt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jq\n",
      "  Using cached jq-1.4.1.tar.gz (2.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: jq\n",
      "  Building wheel for jq (pyproject.toml): started\n",
      "  Building wheel for jq (pyproject.toml): finished with status 'error'\n",
      "Failed to build jq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for jq (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  Executing: ./configure CFLAGS=-fPIC --prefix=C:\\Users\\adinh\\AppData\\Local\\Temp\\pip-install-a8y06sk2\\jq_996a74784b364c019316ced3b01d0e45\\_deps\\build\\onig-install-6.9.4\n",
      "  error: [WinError 2] The system cannot find the file specified\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for jq\n",
      "ERROR: Could not build wheels for jq, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took out JSON from the relevant files since it's not working. I'll try to come back to find a workaround or manually make my own JSON loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_loaders.py\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")\n",
    "from loaders.loader_mapper import LoaderMapper\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"doc, expected\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf', PyMuPDFLoader('tests/docs/dummy_doc_twinkle.pdf')),\n",
    "    ('tests/docs/example.csv', CSVLoader('tests/docs/example.csv')),\n",
    "    ('tests/docs/dummy.txt', TextLoader(file_path='tests/docs/dummy.txt', encoding=\"utf8\")),\n",
    "    ('tests/docs/dummy.html', UnstructuredHTMLLoader('tests/docs/dummy.html')),\n",
    "    ('tests/docs/dummy.md', UnstructuredMarkdownLoader('tests/docs/dummy.md')),\n",
    "    ('tests/docs/dummy.doc', UnstructuredWordDocumentLoader('tests/docs/dummy.doc')),\n",
    "    ('tests/docs/dummy.docx', UnstructuredWordDocumentLoader('tests/docs/dummy.docx')),\n",
    "    ('tests/docs/dummy.pptx', UnstructuredPowerPointLoader('tests/docs/dummy.pptx')),\n",
    "    ('tests/docs/dummy.ppt', UnstructuredPowerPointLoader('tests/docs/dummy.ppt')),\n",
    "    ('tests/docs/dummy.xlsx', UnstructuredExcelLoader('tests/docs/dummy.xlsx')),\n",
    "])\n",
    "def test_return_loader(doc, expected):\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(doc)\n",
    "    assert type(loader) == type(expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- ---------\n",
      "aiohttp                 3.8.4\n",
      "aiosignal               1.3.1\n",
      "anyio                   3.7.0\n",
      "argilla                 1.10.0\n",
      "asttokens               2.2.1\n",
      "async-timeout           4.0.2\n",
      "attrs                   23.1.0\n",
      "backcall                0.2.0\n",
      "backoff                 2.2.1\n",
      "blis                    0.7.9\n",
      "boto3                   1.26.157\n",
      "botocore                1.29.157\n",
      "catalogue               2.0.8\n",
      "certifi                 2023.5.7\n",
      "cffi                    1.15.1\n",
      "chardet                 5.1.0\n",
      "charset-normalizer      3.1.0\n",
      "click                   8.1.3\n",
      "colorama                0.4.6\n",
      "comm                    0.1.3\n",
      "commonmark              0.9.1\n",
      "confection              0.0.4\n",
      "cryptography            41.0.1\n",
      "cymem                   2.0.7\n",
      "dataclasses-json        0.5.8\n",
      "debugpy                 1.6.7\n",
      "decorator               5.1.1\n",
      "deeplake                3.6.3\n",
      "Deprecated              1.2.14\n",
      "dill                    0.3.6\n",
      "distlib                 0.3.6\n",
      "entrypoints             0.4\n",
      "et-xmlfile              1.1.0\n",
      "executing               1.2.0\n",
      "filelock                3.12.2\n",
      "filetype                1.2.0\n",
      "fpdf                    1.7.2\n",
      "frozenlist              1.3.3\n",
      "greenlet                2.0.2\n",
      "h11                     0.14.0\n",
      "httpcore                0.16.3\n",
      "httpx                   0.23.3\n",
      "humbug                  0.3.1\n",
      "idna                    3.4\n",
      "iniconfig               2.0.0\n",
      "ipykernel               6.23.2\n",
      "ipython                 8.14.0\n",
      "jedi                    0.18.2\n",
      "Jinja2                  3.1.2\n",
      "jmespath                1.0.1\n",
      "joblib                  1.2.0\n",
      "jupyter_client          8.2.0\n",
      "jupyter_core            5.3.1\n",
      "langchain               0.0.200\n",
      "langchainplus-sdk       0.0.16\n",
      "langcodes               3.3.0\n",
      "lxml                    4.9.2\n",
      "Markdown                3.4.3\n",
      "MarkupSafe              2.1.3\n",
      "marshmallow             3.19.0\n",
      "marshmallow-enum        1.5.1\n",
      "matplotlib-inline       0.1.6\n",
      "monotonic               1.6\n",
      "msg-parser              1.2.0\n",
      "multidict               6.0.4\n",
      "multiprocess            0.70.14\n",
      "murmurhash              1.0.9\n",
      "mypy-extensions         1.0.0\n",
      "nest-asyncio            1.5.6\n",
      "nltk                    3.8.1\n",
      "numcodecs               0.11.0\n",
      "numexpr                 2.8.4\n",
      "numpy                   1.23.5\n",
      "olefile                 0.46\n",
      "openai                  0.27.8\n",
      "openapi-schema-pydantic 1.2.4\n",
      "openpyxl                3.1.2\n",
      "packaging               23.1\n",
      "pandas                  1.5.3\n",
      "parso                   0.8.3\n",
      "pathos                  0.3.0\n",
      "pathy                   0.10.1\n",
      "pdf2image               1.16.3\n",
      "pdfdocument             4.0.0\n",
      "pdfminer.six            20221105\n",
      "pdfplumber              0.9.0\n",
      "pickleshare             0.7.5\n",
      "Pillow                  9.5.0\n",
      "pip                     23.1.2\n",
      "pipenv                  2023.6.12\n",
      "platformdirs            3.7.0\n",
      "pluggy                  1.2.0\n",
      "pox                     0.3.2\n",
      "ppft                    1.7.6.6\n",
      "preshed                 3.0.8\n",
      "prompt-toolkit          3.0.38\n",
      "psutil                  5.9.5\n",
      "pure-eval               0.2.2\n",
      "pycparser               2.21\n",
      "pydantic                1.10.9\n",
      "Pygments                2.15.1\n",
      "PyJWT                   2.7.0\n",
      "PyMuPDF                 1.22.3\n",
      "pypandoc                1.11\n",
      "pypdf                   3.9.1\n",
      "pypdfium2               4.15.0\n",
      "pytest                  7.3.2\n",
      "python-dateutil         2.8.2\n",
      "python-docx             0.8.11\n",
      "python-magic            0.4.27\n",
      "python-pptx             0.6.21\n",
      "pytz                    2023.3\n",
      "pywin32                 306\n",
      "PyYAML                  6.0\n",
      "pyzmq                   25.1.0\n",
      "regex                   2023.6.3\n",
      "reportlab               4.0.4\n",
      "requests                2.31.0\n",
      "rfc3986                 1.5.0\n",
      "rich                    13.0.1\n",
      "s3transfer              0.6.1\n",
      "setuptools              68.0.0\n",
      "six                     1.16.0\n",
      "smart-open              6.3.0\n",
      "sniffio                 1.3.0\n",
      "spacy                   3.5.3\n",
      "spacy-legacy            3.0.12\n",
      "spacy-loggers           1.0.4\n",
      "SQLAlchemy              2.0.16\n",
      "srsly                   2.4.6\n",
      "stack-data              0.6.2\n",
      "tabulate                0.9.0\n",
      "tenacity                8.2.2\n",
      "thinc                   8.1.10\n",
      "tiktoken                0.4.0\n",
      "tornado                 6.3.2\n",
      "tqdm                    4.65.0\n",
      "traitlets               5.9.0\n",
      "typer                   0.9.0\n",
      "typing_extensions       4.6.3\n",
      "typing-inspect          0.9.0\n",
      "unstructured            0.7.4\n",
      "urllib3                 1.26.16\n",
      "virtualenv              20.23.0\n",
      "virtualenv-clone        0.5.7\n",
      "Wand                    0.6.11\n",
      "wasabi                  1.1.2\n",
      "wcwidth                 0.2.6\n",
      "wheel                   0.40.0\n",
      "wrapt                   1.14.1\n",
      "xlrd                    2.0.1\n",
      "XlsxWriter              3.1.2\n",
      "yarl                    1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 10 items\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 2.07s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/test_loaders.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the actual file loading capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to tests/test_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "@pytest.mark.parametrize(\"doc, expected\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\"),\n",
    "    ('tests/docs/example.csv', \n",
    "     \"\"\"Name: John\n",
    "        Age: 25\n",
    "        Country: USA\"\"\"),\n",
    "    ('tests/docs/dummy.txt',\n",
    "    \"\"\"Blah blah blah. Sample text. Blah Blah\n",
    "    Blah Blah Blah. This is so fun. Blah Blah.\n",
    "    Abcdefghijklmnopqrstuvwxyz.\"\"\" ),\n",
    "    ('tests/docs/dummy.html', \n",
    "    \"\"\"This is a dummy HTML file.\n",
    "    It serves as an example.\"\"\"),\n",
    "    ('tests/docs/dummy.md', \n",
    "    \"\"\"Dummy Markdown File\n",
    "    This is a dummy Markdown file.\n",
    "    It serves as an example. Item 1 Item 2 Item 3\"\"\"),\n",
    "    ('tests/docs/dummy.docx',\n",
    "    \"\"\"Dummy Document\n",
    "    This is a dummy Word document.\"\"\"),\n",
    "    ('tests/docs/dummy.pptx',\n",
    "    \"\"\"Dummy Presentation\n",
    "    This is a dummy PowerPoint presentation.\"\"\"),\n",
    "    ('tests/docs/dummy.xlsx',\n",
    "    \"\"\"This is a dummy Excel spreadsheet.\"\"\"),\n",
    "])\n",
    "def test_load_doc(doc, expected):\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(doc)\n",
    "    loaded_doc = loader.load()\n",
    "    text = loaded_doc[0].page_content\n",
    "    actual_normalized = re.sub(r'\\s+', ' ', text.strip())\n",
    "    expected_normalized = re.sub(r'\\s+', ' ', expected.strip())\n",
    "    assert actual_normalized == expected_normalized\n",
    "    \n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 16 items\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m16 passed\u001b[0m\u001b[32m in 3.76s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/test_loaders.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another note: I went back and removed the functionality for doc and ppt in favor of docx and pptx because it was outdated and caused issues(required me to install more things on my computer). I didn't want to overcomplicate this for anyone wanting to use it, so unless it's absolutely necessary, I won't be implementing it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to go across the repo and change the imports for the text loader to use the new loader mapper instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 20 items\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                   [ 80%]\u001b[0m\n",
      "tests\\test_splitters.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m20 passed\u001b[0m\u001b[32m in 4.15s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/test_splitters.py\n",
    "\n",
    "from loaders.loader_mapper import LoaderMapper\n",
    "from splitters import character, nltk, recursive, tiktoken\n",
    "import pytest\n",
    "\n",
    "#difficult to formally compare results of text splitters, so I checked visually that it worked beforehand, and just tested to see that imports work correctly.\n",
    "mapper = LoaderMapper()\n",
    "loader = mapper.find_loader('tests/docs/dummy_doc_twinkle.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "def test_character():\n",
    "    splitter = character.Character_TextSplitter(\n",
    "        separator= \"\\n\",\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len,\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_ntlk():\n",
    "    splitter = nltk.NLTK_TextSplitter(chunk_size = 1000)\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_recursive():\n",
    "    splitter = recursive.RecursiveCharacter_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_tiktoken():\n",
    "    splitter = tiktoken.Token_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "from loaders import pdfplumber, pymupdf, pypdf, pypdf2, pypdfium2, unstructured\n",
    "import pytest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we'll use a small dummy pdf for our tests, not the instruction manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fpdf\n",
    "from fpdf import FPDF\n",
    "\n",
    "\n",
    "pdf = FPDF()\n",
    "\n",
    "pdf.add_page()\n",
    "\n",
    "pdf.set_font('Arial', size=12)\n",
    "\n",
    "text = \"\"\"Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\n",
    "Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\"\"\"\n",
    "\n",
    "pdf.multi_cell(0, 10, txt=text)\n",
    "\n",
    "pdf.output('tests/dummy_doc_twinkle.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a test for the pdfplumber class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "\n",
    "@pytest.mark.parametrize(\"pdf, expected\", [\n",
    "    ('tests/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\")\n",
    "])\n",
    "def test_plumber(pdf, expected):\n",
    "    loader = pdfplumber.pdfplumber_loader(pdf)\n",
    "    data = loader.load_text()\n",
    "    assert data[0].page_content.strip() == expected.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pytest -vv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick note, I removed pypdf2 because it is deprecated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets write a test for pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "\n",
    "@pytest.mark.parametrize(\"pdf, expected\", [\n",
    "    ('tests/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\")\n",
    "])\n",
    "def test_pymu(pdf, expected):\n",
    "    loader = pymupdf.PyMuPDF_Loader(pdf)\n",
    "    data = loader.load_text()\n",
    "    assert data[0].page_content.strip() == expected.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "\n",
    "@pytest.mark.parametrize(\"pdf, expected\", [\n",
    "    ('tests/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\")\n",
    "])\n",
    "def test_pypdf(pdf, expected):\n",
    "    loader = pypdf.PyPDF_Loader(pdf)\n",
    "    data = loader.load_text()\n",
    "    assert data[0].page_content.strip() == expected.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pypdfium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "\n",
    "#this pdf loader is a little special, so normalize the text extracted before similarity comparison\n",
    "import re\n",
    "@pytest.mark.parametrize(\"pdf, expected\", [\n",
    "    ('tests/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\n",
    "Up above the world so high,\n",
    "Like a diamond in the sky.\n",
    "Twinkle, twinkle, little star,\n",
    "How I wonder what you are!\"\"\")\n",
    "])\n",
    "def test_pypdfium(pdf, expected):\n",
    "    loader = pypdfium2.PyPDFium2PDF_Loader(pdf)\n",
    "    data = loader.load_text()\n",
    "    \n",
    "    # Normalize line endings and remove extra whitespaces\n",
    "    actual_normalized = re.sub(r'\\s+', ' ', data[0].page_content.strip())\n",
    "    expected_normalized = re.sub(r'\\s+', ' ', expected.strip())\n",
    "    \n",
    "    assert actual_normalized == expected_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, make a test for unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "\n",
    "@pytest.mark.parametrize(\"pdf, expected\", [\n",
    "    ('tests/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\")\n",
    "])\n",
    "def test_unstructured(pdf, expected):\n",
    "    loader = unstructured.UnstructuredPDF_Loader(pdf)\n",
    "    data = loader.load_text()\n",
    "    actual_normalized = re.sub(r'\\s+', ' ', data[0].page_content.strip())\n",
    "    expected_normalized = re.sub(r'\\s+', ' ', expected.strip())\n",
    "    \n",
    "    assert actual_normalized == expected_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's write up some tests for the text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_splitters.py\n",
    "from loaders import pymupdf\n",
    "from splitters import character, nltk, recursive, tiktoken\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a tests/test_splitters.py\n",
    "\n",
    "loader = pymupdf.PyMuPDF_Loader('tests/dummy_doc_twinkle.pdf')\n",
    "data = loader.load_text()\n",
    "\n",
    "def test_character():\n",
    "    splitter = character.Character_TextSplitter(\n",
    "        separator= \"\\n\",\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len,\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_ntlk():\n",
    "    splitter = nltk.NLTK_TextSplitter(chunk_size = 1000)\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_recursive():\n",
    "    splitter = recursive.RecursiveCharacter_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_tiktoken():\n",
    "    splitter = tiktoken.Token_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, I want to account for different file types, not just PDF. So I'm gonna have to adjust the loaders subdirectory quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading stage is what handles extracting text from different file types. So let's group all the pdf loaders into a single subdirectory called pdf in loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#make the directory \"pdf\"\n",
    "Path(\"loaders/pdf\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loaders/pdf\\\\unstructured.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.move('loaders/pdfplumber.py', 'loaders/pdf')\n",
    "shutil.move('loaders/pymupdf.py', 'loaders/pdf')\n",
    "shutil.move('loaders/pypdf.py', 'loaders/pdf')\n",
    "shutil.move('loaders/pypdfium2.py', 'loaders/pdf')\n",
    "shutil.move('loaders/unstructured.py', 'loaders/pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's make a main pdf loader module to handle the different file types and load accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing loaders/multi_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile loaders/loader_mapper.py\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to loaders/multi_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a loaders/loader_mapper.py\n",
    "\n",
    "class LoaderMapper:\n",
    "    \"\"\"\n",
    "    LoaderMapper can accept multiple file types and return a langchain loader wrapper that corresponds to the associated loader.\n",
    "    Currently supports csv, pdf, txt, html, md, doc, docx, ppt, pptx, xls, xlsx, json\n",
    "    \"\"\"\n",
    "    #keep dict of file extensions and their relevant loaders with their arguments\n",
    "    loader_map = {\n",
    "            \".csv\": (CSVLoader, {}),\n",
    "            \".pdf\": (PyMuPDFLoader, {}),\n",
    "            \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "            \".html\": (UnstructuredHTMLLoader, {}),\n",
    "            \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "            \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "            \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "            \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "            \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "            \".xls\": (UnstructuredExcelLoader, {}),\n",
    "            \".xlsx\": (UnstructuredExcelLoader, {}),\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def find_loader(self, filepath):\n",
    "        \"\"\"\n",
    "        Finds the associated loader based on filepath extension\n",
    "        \n",
    "        :param filepath: path of the file to be loaded\n",
    "        \n",
    "        :return: langchain loader wrapper object\n",
    "        \"\"\"\n",
    "        ext = \".\" + filepath.rsplit(\".\", 1)[-1]\n",
    "        if ext in LoaderMapper.loader_map:\n",
    "            loader_class, loader_args = LoaderMapper.loader_map[ext]\n",
    "            loader = loader_class(filepath, **loader_args)\n",
    "            return loader\n",
    "        \n",
    "        raise ValueError(f\"Unsupported file extension '{ext}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test all the different file types to see if the class works as intended. We'll use the test_loaders.py module and pytest again. We also need to update the import line since we moved the pdf loaders under loaders/pdf. UPDATE: I just made a change to the multi_loader class and changed it to LoaderMapper instead. I want to return the relevant loader instead of doing all the loading work in that class. So I think I'll be removing the pdf directory altogether since we only need one pdf loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "directory = 'loaders/pdf'\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory):\n",
    "    # Remove the directory and its contents\n",
    "    shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all the old tests with the pdf loaders and write new ones. One for returning the correct loader and another for actually returning the loaded text. Parametrize for each file type: pdf, json, docx etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_loaders.py\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")\n",
    "from loaders.loader_mapper import LoaderMapper\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"doc, expected\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf', PyMuPDFLoader('tests/docs/dummy_doc_twinkle.pdf'))\n",
    "])\n",
    "def test_return_loader(doc, expected):\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(doc)\n",
    "    assert type(loader) == type(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 1 item\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 2.00s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/test_loaders.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more tests for each file type. I'm going to need to make dummy files for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tests/docs\\\\dummy_doc_twinkle.pdf'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(\"tests/docs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import csv\n",
    "\n",
    "data = [\n",
    "    ['Name', 'Age', 'Country'],\n",
    "    ['John', 25, 'USA'],\n",
    "    ['Alice', 30, 'Canada'],\n",
    "    ['Bob', 35, 'Australia']\n",
    "]\n",
    "\n",
    "filename = 'tests/docs/example.csv'\n",
    "\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n",
    "import shutil\n",
    "shutil.move('tests/dummy_doc_twinkle.pdf', 'tests/docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"tests/docs/dummy.txt\"\n",
    "\n",
    "content=\"\"\"Blah blah blah. Sample text. Blah Blah\n",
    "Blah Blah Blah. This is so fun. Blah Blah.\n",
    "Abcdefghijklmnopqrstuvwxyz.\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as file: \n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/docs/dummy.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/docs/dummy.html\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Dummy HTML File</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>This is a dummy HTML file.</h1>\n",
    "    <p>It serves as an example.</p>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/docs/dummy.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/docs/dummy.md\n",
    "\n",
    "# Dummy Markdown File\n",
    "\n",
    "This is a dummy Markdown file.\n",
    "It serves as an example.\n",
    "\n",
    "- Item 1\n",
    "- Item 2\n",
    "- Item 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Dummy Document', 0)\n",
    "document.add_paragraph('This is a dummy Word document.')\n",
    "\n",
    "document.save('tests/docs/dummy.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "from docx.shared import Inches\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Dummy Document', 0)\n",
    "paragraph = document.add_paragraph('This is a dummy Word document.')\n",
    "paragraph.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "\n",
    "document.save('tests/docs/dummy.doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#powerpoint\n",
    "from pptx import Presentation\n",
    "\n",
    "presentation = Presentation()\n",
    "\n",
    "slide_1 = presentation.slides.add_slide(presentation.slide_layouts[0])\n",
    "title_1 = slide_1.shapes.title\n",
    "subtitle_1 = slide_1.placeholders[1]\n",
    "\n",
    "title_1.text = \"Dummy Presentation\"\n",
    "subtitle_1.text = \"This is a dummy PowerPoint presentation.\"\n",
    "\n",
    "presentation.save(\"tests/docs/dummy.pptx\")\n",
    "\n",
    "#excel\n",
    "import openpyxl\n",
    "\n",
    "workbook = openpyxl.Workbook()\n",
    "worksheet = workbook.active\n",
    "\n",
    "worksheet.title = \"Dummy Sheet\"\n",
    "worksheet[\"A1\"] = \"Dummy Excel Spreadsheet\"\n",
    "worksheet[\"A2\"] = \"This is a dummy Excel spreadsheet.\"\n",
    "\n",
    "workbook.save(\"tests/docs/dummy.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "# Create a presentation object\n",
    "presentation = Presentation()\n",
    "\n",
    "# Add a slide with a title and content\n",
    "slide_layout = presentation.slide_layouts[1]  # Slide layout with title and content\n",
    "slide = presentation.slides.add_slide(slide_layout)\n",
    "title = slide.shapes.title\n",
    "content = slide.placeholders[1]\n",
    "\n",
    "title.text = \"Dummy Slide\"\n",
    "content.text = \"This is a dummy PowerPoint slide.\"\n",
    "\n",
    "# Save the presentation to a PPT file\n",
    "presentation.save(\"tests/docs/dummy.ppt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jq\n",
      "  Using cached jq-1.4.1.tar.gz (2.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: jq\n",
      "  Building wheel for jq (pyproject.toml): started\n",
      "  Building wheel for jq (pyproject.toml): finished with status 'error'\n",
      "Failed to build jq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for jq (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [5 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_ext\n",
      "  Executing: ./configure CFLAGS=-fPIC --prefix=C:\\Users\\adinh\\AppData\\Local\\Temp\\pip-install-a8y06sk2\\jq_996a74784b364c019316ced3b01d0e45\\_deps\\build\\onig-install-6.9.4\n",
      "  error: [WinError 2] The system cannot find the file specified\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for jq\n",
      "ERROR: Could not build wheels for jq, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took out JSON from the relevant files since it's not working. I'll try to come back to find a workaround or manually make my own JSON loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_loaders.py\n",
    "from langchain.document_loaders import (\n",
    "    CSVLoader,\n",
    "    PyMuPDFLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredExcelLoader,\n",
    ")\n",
    "from loaders.loader_mapper import LoaderMapper\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"doc, expected\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf', PyMuPDFLoader('tests/docs/dummy_doc_twinkle.pdf')),\n",
    "    ('tests/docs/example.csv', CSVLoader('tests/docs/example.csv')),\n",
    "    ('tests/docs/dummy.txt', TextLoader(file_path='tests/docs/dummy.txt', encoding=\"utf8\")),\n",
    "    ('tests/docs/dummy.html', UnstructuredHTMLLoader('tests/docs/dummy.html')),\n",
    "    ('tests/docs/dummy.md', UnstructuredMarkdownLoader('tests/docs/dummy.md')),\n",
    "    ('tests/docs/dummy.doc', UnstructuredWordDocumentLoader('tests/docs/dummy.doc')),\n",
    "    ('tests/docs/dummy.docx', UnstructuredWordDocumentLoader('tests/docs/dummy.docx')),\n",
    "    ('tests/docs/dummy.pptx', UnstructuredPowerPointLoader('tests/docs/dummy.pptx')),\n",
    "    ('tests/docs/dummy.ppt', UnstructuredPowerPointLoader('tests/docs/dummy.ppt')),\n",
    "    ('tests/docs/dummy.xlsx', UnstructuredExcelLoader('tests/docs/dummy.xlsx')),\n",
    "])\n",
    "def test_return_loader(doc, expected):\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(doc)\n",
    "    assert type(loader) == type(expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                 Version\n",
      "----------------------- ---------\n",
      "aiohttp                 3.8.4\n",
      "aiosignal               1.3.1\n",
      "anyio                   3.7.0\n",
      "argilla                 1.10.0\n",
      "asttokens               2.2.1\n",
      "async-timeout           4.0.2\n",
      "attrs                   23.1.0\n",
      "backcall                0.2.0\n",
      "backoff                 2.2.1\n",
      "blis                    0.7.9\n",
      "boto3                   1.26.157\n",
      "botocore                1.29.157\n",
      "catalogue               2.0.8\n",
      "certifi                 2023.5.7\n",
      "cffi                    1.15.1\n",
      "chardet                 5.1.0\n",
      "charset-normalizer      3.1.0\n",
      "click                   8.1.3\n",
      "colorama                0.4.6\n",
      "comm                    0.1.3\n",
      "commonmark              0.9.1\n",
      "confection              0.0.4\n",
      "cryptography            41.0.1\n",
      "cymem                   2.0.7\n",
      "dataclasses-json        0.5.8\n",
      "debugpy                 1.6.7\n",
      "decorator               5.1.1\n",
      "deeplake                3.6.3\n",
      "Deprecated              1.2.14\n",
      "dill                    0.3.6\n",
      "distlib                 0.3.6\n",
      "entrypoints             0.4\n",
      "et-xmlfile              1.1.0\n",
      "executing               1.2.0\n",
      "filelock                3.12.2\n",
      "filetype                1.2.0\n",
      "fpdf                    1.7.2\n",
      "frozenlist              1.3.3\n",
      "greenlet                2.0.2\n",
      "h11                     0.14.0\n",
      "httpcore                0.16.3\n",
      "httpx                   0.23.3\n",
      "humbug                  0.3.1\n",
      "idna                    3.4\n",
      "iniconfig               2.0.0\n",
      "ipykernel               6.23.2\n",
      "ipython                 8.14.0\n",
      "jedi                    0.18.2\n",
      "Jinja2                  3.1.2\n",
      "jmespath                1.0.1\n",
      "joblib                  1.2.0\n",
      "jupyter_client          8.2.0\n",
      "jupyter_core            5.3.1\n",
      "langchain               0.0.200\n",
      "langchainplus-sdk       0.0.16\n",
      "langcodes               3.3.0\n",
      "lxml                    4.9.2\n",
      "Markdown                3.4.3\n",
      "MarkupSafe              2.1.3\n",
      "marshmallow             3.19.0\n",
      "marshmallow-enum        1.5.1\n",
      "matplotlib-inline       0.1.6\n",
      "monotonic               1.6\n",
      "msg-parser              1.2.0\n",
      "multidict               6.0.4\n",
      "multiprocess            0.70.14\n",
      "murmurhash              1.0.9\n",
      "mypy-extensions         1.0.0\n",
      "nest-asyncio            1.5.6\n",
      "nltk                    3.8.1\n",
      "numcodecs               0.11.0\n",
      "numexpr                 2.8.4\n",
      "numpy                   1.23.5\n",
      "olefile                 0.46\n",
      "openai                  0.27.8\n",
      "openapi-schema-pydantic 1.2.4\n",
      "openpyxl                3.1.2\n",
      "packaging               23.1\n",
      "pandas                  1.5.3\n",
      "parso                   0.8.3\n",
      "pathos                  0.3.0\n",
      "pathy                   0.10.1\n",
      "pdf2image               1.16.3\n",
      "pdfdocument             4.0.0\n",
      "pdfminer.six            20221105\n",
      "pdfplumber              0.9.0\n",
      "pickleshare             0.7.5\n",
      "Pillow                  9.5.0\n",
      "pip                     23.1.2\n",
      "pipenv                  2023.6.12\n",
      "platformdirs            3.7.0\n",
      "pluggy                  1.2.0\n",
      "pox                     0.3.2\n",
      "ppft                    1.7.6.6\n",
      "preshed                 3.0.8\n",
      "prompt-toolkit          3.0.38\n",
      "psutil                  5.9.5\n",
      "pure-eval               0.2.2\n",
      "pycparser               2.21\n",
      "pydantic                1.10.9\n",
      "Pygments                2.15.1\n",
      "PyJWT                   2.7.0\n",
      "PyMuPDF                 1.22.3\n",
      "pypandoc                1.11\n",
      "pypdf                   3.9.1\n",
      "pypdfium2               4.15.0\n",
      "pytest                  7.3.2\n",
      "python-dateutil         2.8.2\n",
      "python-docx             0.8.11\n",
      "python-magic            0.4.27\n",
      "python-pptx             0.6.21\n",
      "pytz                    2023.3\n",
      "pywin32                 306\n",
      "PyYAML                  6.0\n",
      "pyzmq                   25.1.0\n",
      "regex                   2023.6.3\n",
      "reportlab               4.0.4\n",
      "requests                2.31.0\n",
      "rfc3986                 1.5.0\n",
      "rich                    13.0.1\n",
      "s3transfer              0.6.1\n",
      "setuptools              68.0.0\n",
      "six                     1.16.0\n",
      "smart-open              6.3.0\n",
      "sniffio                 1.3.0\n",
      "spacy                   3.5.3\n",
      "spacy-legacy            3.0.12\n",
      "spacy-loggers           1.0.4\n",
      "SQLAlchemy              2.0.16\n",
      "srsly                   2.4.6\n",
      "stack-data              0.6.2\n",
      "tabulate                0.9.0\n",
      "tenacity                8.2.2\n",
      "thinc                   8.1.10\n",
      "tiktoken                0.4.0\n",
      "tornado                 6.3.2\n",
      "tqdm                    4.65.0\n",
      "traitlets               5.9.0\n",
      "typer                   0.9.0\n",
      "typing_extensions       4.6.3\n",
      "typing-inspect          0.9.0\n",
      "unstructured            0.7.4\n",
      "urllib3                 1.26.16\n",
      "virtualenv              20.23.0\n",
      "virtualenv-clone        0.5.7\n",
      "Wand                    0.6.11\n",
      "wasabi                  1.1.2\n",
      "wcwidth                 0.2.6\n",
      "wheel                   0.40.0\n",
      "wrapt                   1.14.1\n",
      "xlrd                    2.0.1\n",
      "XlsxWriter              3.1.2\n",
      "yarl                    1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 10 items\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m10 passed\u001b[0m\u001b[32m in 2.07s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/test_loaders.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the actual file loading capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to tests/test_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a tests/test_loaders.py\n",
    "@pytest.mark.parametrize(\"doc, expected\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\"),\n",
    "    ('tests/docs/example.csv', \n",
    "     \"\"\"Name: John\n",
    "        Age: 25\n",
    "        Country: USA\"\"\"),\n",
    "    ('tests/docs/dummy.txt',\n",
    "    \"\"\"Blah blah blah. Sample text. Blah Blah\n",
    "    Blah Blah Blah. This is so fun. Blah Blah.\n",
    "    Abcdefghijklmnopqrstuvwxyz.\"\"\" ),\n",
    "    ('tests/docs/dummy.html', \n",
    "    \"\"\"This is a dummy HTML file.\n",
    "    It serves as an example.\"\"\"),\n",
    "    ('tests/docs/dummy.md', \n",
    "    \"\"\"Dummy Markdown File\n",
    "    This is a dummy Markdown file.\n",
    "    It serves as an example. Item 1 Item 2 Item 3\"\"\"),\n",
    "    ('tests/docs/dummy.docx',\n",
    "    \"\"\"Dummy Document\n",
    "    This is a dummy Word document.\"\"\"),\n",
    "    ('tests/docs/dummy.pptx',\n",
    "    \"\"\"Dummy Presentation\n",
    "    This is a dummy PowerPoint presentation.\"\"\"),\n",
    "    ('tests/docs/dummy.xlsx',\n",
    "    \"\"\"This is a dummy Excel spreadsheet.\"\"\"),\n",
    "])\n",
    "def test_load_doc(doc, expected):\n",
    "    mapper = LoaderMapper()\n",
    "    loader = mapper.find_loader(doc)\n",
    "    loaded_doc = loader.load()\n",
    "    text = loaded_doc[0].page_content\n",
    "    actual_normalized = re.sub(r'\\s+', ' ', text.strip())\n",
    "    expected_normalized = re.sub(r'\\s+', ' ', expected.strip())\n",
    "    assert actual_normalized == expected_normalized\n",
    "    \n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 16 items\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m16 passed\u001b[0m\u001b[32m in 3.76s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/test_loaders.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another note: I went back and removed the functionality for doc and ppt in favor of docx and pptx because it was outdated and caused issues(required me to install more things on my computer). I didn't want to overcomplicate this for anyone wanting to use it, so unless it's absolutely necessary, I won't be implementing it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to go across the repo and change the imports for the text loader to use the new loader mapper instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 20 items\n",
      "\n",
      "tests\\test_loaders.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                   [ 80%]\u001b[0m\n",
      "tests\\test_splitters.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m20 passed\u001b[0m\u001b[32m in 4.15s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/test_splitters.py\n",
    "\n",
    "from loaders.loader_mapper import LoaderMapper\n",
    "from splitters import character, nltk, recursive, tiktoken\n",
    "import pytest\n",
    "\n",
    "#difficult to formally compare results of text splitters, so I checked visually that it worked beforehand, and just tested to see that imports work correctly.\n",
    "mapper = LoaderMapper()\n",
    "loader = mapper.find_loader('tests/docs/dummy_doc_twinkle.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "def test_character():\n",
    "    splitter = character.Character_TextSplitter(\n",
    "        separator= \"\\n\",\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len,\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_ntlk():\n",
    "    splitter = nltk.NLTK_TextSplitter(chunk_size = 1000)\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_recursive():\n",
    "    splitter = recursive.RecursiveCharacter_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n",
    "\n",
    "def test_tiktoken():\n",
    "    splitter = tiktoken.Token_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "    )\n",
    "    split_docs = splitter.split_data(data)\n",
    "    assert split_docs is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/test_vectorstore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_vectorstore.py\n",
    "import pytest\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from ai.embeddings.embeddings_mapper import Embeddings_Mapper\n",
    "from utils.loaders.loader_mapper import LoaderMapper\n",
    "from utils.splitters.recursive import RecursiveCharacter_TextSplitter\n",
    "from utils.vectorstores.deep_lake import DeeplakeDB\n",
    "\n",
    "def clear_db():\n",
    "    embeddings_mapper = Embeddings_Mapper()\n",
    "    embeddings = embeddings_mapper.find_model(\"openai\")\n",
    "    deeplake = DeeplakeDB(store_path = './test_deeplake', embedding_model = embeddings)\n",
    "    deeplake.delete_all()\n",
    "\n",
    "@pytest.fixture(scope=\"session\", autouse=True)\n",
    "def teardown(request):\n",
    "    request.addfinalizer(clear_db)\n",
    "    \n",
    "@pytest.mark.parametrize(\"file, content\", [\n",
    "    ('tests/docs/dummy_doc_twinkle.pdf',\n",
    "    \"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\"),\n",
    "    ('tests/docs/example.csv', \n",
    "     \"\"\"Name: John\n",
    "        Age: 25\n",
    "        Country: USA\"\"\"),\n",
    "    ('tests/docs/dummy.txt',\n",
    "    \"\"\"Blah blah blah. Sample text. Blah Blah\n",
    "    Blah Blah Blah. This is so fun. Blah Blah.\n",
    "    Abcdefghijklmnopqrstuvwxyz.\"\"\" ),\n",
    "    ('tests/docs/dummy.html', \n",
    "    \"\"\"This is a dummy HTML file.\n",
    "    It serves as an example.\"\"\"),\n",
    "    ('tests/docs/dummy.md', \n",
    "    \"\"\"Dummy Markdown File\n",
    "    This is a dummy Markdown file.\n",
    "    It serves as an example. Item 1 Item 2 Item 3\"\"\"),\n",
    "    ('tests/docs/dummy.docx',\n",
    "    \"\"\"Dummy Document\n",
    "    This is a dummy Word document.\"\"\"),\n",
    "    ('tests/docs/dummy.pptx',\n",
    "    \"\"\"Dummy Presentation\n",
    "    This is a dummy PowerPoint presentation.\"\"\"),\n",
    "    ('tests/docs/dummy.xlsx',\n",
    "    \"\"\"This is a dummy Excel spreadsheet.\"\"\"),\n",
    "])\n",
    "def test_deeplake(file, content):\n",
    "    #set up document to be embedded and stored\n",
    "    loadermapper = LoaderMapper()\n",
    "    loader = loadermapper.find_loader(file)\n",
    "    data = loader.load()\n",
    "    splitter = RecursiveCharacter_TextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len\n",
    "    )\n",
    "    docs = [splitter.split_data(data)]\n",
    "    #set up deeplake db and pass in relevant params\n",
    "    embeddings_mapper = Embeddings_Mapper()\n",
    "    embeddings = embeddings_mapper.find_model(\"huggingface\")\n",
    "    deeplake = DeeplakeDB(store_path = './test_deeplake', embedding_model = embeddings)\n",
    "    deeplake.add_docs(docs)\n",
    "    #pass in the file contents and see if it can return the most relevant document\n",
    "    doc = deeplake.find_similar(content)\n",
    "    source = doc[0].metadata[\"source\"]\n",
    "    assert file == source\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.3.2, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 8 items\n",
      "\n",
      "tests\\test_vectorstore.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== warnings summary ===============================\u001b[0m\n",
      "tests/test_vectorstore.py::test_deeplake[tests/docs/dummy_doc_twinkle.pdf-Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!]\n",
      "  C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\humbug\\report.py:47: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "    import pkg_resources  # type: ignore\n",
      "\n",
      "tests/test_vectorstore.py::test_deeplake[tests/docs/dummy_doc_twinkle.pdf-Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!]\n",
      "  C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.6) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "    warnings.warn(\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m======================= \u001b[32m8 passed\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 16.42s\u001b[0m\u001b[33m ========================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-38:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-40:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-41:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-42:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-44:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-46:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-47:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-49:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n",
      "Exception in thread Thread-50:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 77, in run\n",
      "    self._hide_cursor()\n",
      "  File \"C:\\Users\\adinh\\.virtualenvs\\utils-Z4NtiPxo\\Lib\\site-packages\\deeplake\\util\\spinner.py\", line 119, in _hide_cursor\n",
      "    if self.file.isatty():\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adinh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py\", line 483, in func_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: I/O operation on closed file\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_vectorstore.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to make an AWS lambda function to automate my training process. It will be triggered every time a document is upladed to s3 or microsoft sharepoint. But the issue is, while I have the training logic implemented, I need to have a cloud based vector store since the lambda function wont have access to my local file system since it is not ran locally. It is ran automatically whenever the trigger event occurs, so I need to set up a cloud vector store, Pinecone for this lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vectorstores/pinecone.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vectorstores/pinecone.py\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "class Pinecone:\n",
    "    def __init__(self, index, embedding_model):\n",
    "        self.db = Pinecone(index=index, embedding_function = embedding_model)\n",
    "    \n",
    "    def add_docs(self, documents):\n",
    "        ids = []\n",
    "        for document in documents:\n",
    "            id = self.db.add_documents(document)\n",
    "            ids.append(id)\n",
    "        if len(ids) == 1:\n",
    "            return ids[0]\n",
    "        return ids\n",
    "    \n",
    "    def find_similar(self, query):\n",
    "        return self.db.similarity_search(query)\n",
    "    \n",
    "    def delete_by_ids(self, ids):\n",
    "        self.db.delete(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a test for it and see if this implementation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.4.0, pluggy-1.2.0\n",
      "rootdir: C:\\Users\\adinh\\Downloads\\ctlpchatbot\\utils\n",
      "plugins: anyio-3.7.0\n",
      "collected 1 item\n",
      "\n",
      "tests\\test_vectorstore.py \u001b[31mF\u001b[0m\u001b[31m                                              [100%]\u001b[0m\n",
      "\n",
      "================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_ test_deeplake[tests/docs/dummy_doc_twinkle.pdf-Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!] _\u001b[0m\n",
      "\n",
      "file = 'tests/docs/dummy_doc_twinkle.pdf'\n",
      "content = 'Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!'\n",
      "\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfile, content\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[90m\u001b[39;49;00m\n",
      "        (\u001b[33m'\u001b[39;49;00m\u001b[33mtests/docs/dummy_doc_twinkle.pdf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!\"\"\"\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "    ])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_deeplake\u001b[39;49;00m(file, content):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#set up document to be embedded and stored\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        loadermapper = LoaderMapper()\u001b[90m\u001b[39;49;00m\n",
      "        loader = loadermapper.find_loader(file)\u001b[90m\u001b[39;49;00m\n",
      "        data = loader.load()\u001b[90m\u001b[39;49;00m\n",
      "        splitter = RecursiveCharacter_TextSplitter(\u001b[90m\u001b[39;49;00m\n",
      "            chunk_size = \u001b[94m1000\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            chunk_overlap = \u001b[94m200\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            length_function = \u001b[96mlen\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "        docs = [splitter.split_data(data)]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m#set up deeplake db and pass in relevant params\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        embeddings_mapper = Embeddings_Mapper()\u001b[90m\u001b[39;49;00m\n",
      "        embeddings = embeddings_mapper.find_model(\u001b[33m\"\u001b[39;49;00m\u001b[33mhuggingface\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       pinecone = Pinecone(index = \u001b[33m\"\u001b[39;49;00m\u001b[33mtest1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, embedding_model = embeddings)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests\\test_vectorstore.py\u001b[0m:28: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "self = <utils.vectorstores.pinecone.Pinecone object at 0x000002D86BC9E510>\n",
      "index = 'test1'\n",
      "embedding_model = HuggingFaceEmbeddings(client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) ...malize()\n",
      "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={})\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92m__init__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, index, embedding_model):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[96mself\u001b[39;49;00m.db = Pinecone(index=index, embedding_function = embedding_model)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: Pinecone.__init__() got an unexpected keyword argument 'embedding_function'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mvectorstores\\pinecone.py\u001b[0m:5: TypeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_vectorstore.py::\u001b[1mtest_deeplake[tests/docs/dummy_doc_twinkle.pdf-Twinkle, twinkle, little star,\\nHow I wonder what you are!\\nUp above the world so high,\\nLike a diamond in the sky.\\nTwinkle, twinkle, little star,\\nHow I wonder what you are!]\u001b[0m - TypeError: Pinecone.__init__() got an unexpected keyword argument 'embeddin...\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 8.65s\u001b[0m\u001b[31m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest tests/test_vectorstore.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
